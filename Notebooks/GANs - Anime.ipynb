{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Preprocessing-Images\" data-toc-modified-id=\"Preprocessing-Images-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Preprocessing Images</a></span></li><li><span><a href=\"#GANs:-Discrimator-and-Generator\" data-toc-modified-id=\"GANs:-Discrimator-and-Generator-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>GANs: Discrimator and Generator</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GANs for Anime Faces\n",
    "\n",
    "The main idea of this notebook is to use GANs in order to generate fake anime faces. I would use the tutorial [GANs from Scratch 1: A deep introduction. With code in PyTorch and TensorFlow](https://medium.com/ai-society/gans-from-scratch-1-a-deep-introduction-with-code-in-pytorch-and-tensorflow-cb03cdcdba0f).\n",
    "\n",
    "By the end of this little project, I hope to have some realistic anime faces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-23T15:05:01.152556Z",
     "start_time": "2020-03-23T15:04:58.777647Z"
    }
   },
   "outputs": [],
   "source": [
    "#Loading the required libraries\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.autograd.variable import Variable\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "import glob\n",
    "from PIL import Image\n",
    "# from utils import Logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Images\n",
    "\n",
    "This section contains the reading and preprocessing of the anime faces obtained in [Anime Faces](https://www.kaggle.com/soumikrakshit/anime-faces/data) and the ones thanks to [Brian Chao](https://github.com/Mckinsey666) and his repostory [Anime-Face-Dataset](https://github.com/Mckinsey666/Anime-Face-Dataset). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-23T15:05:01.231275Z",
     "start_time": "2020-03-23T15:05:01.154655Z"
    }
   },
   "outputs": [],
   "source": [
    "def mnist_data():\n",
    "    compose = transforms.Compose(\n",
    "        [transforms.ToTensor(),\n",
    "         transforms.Normalize((.5, .5, .5), (.5, .5, .5))\n",
    "        ])\n",
    "    out_dir = './dataset'\n",
    "    return datasets.MNIST(root=out_dir, train=True, transform=compose, download=True)\n",
    "# Load data\n",
    "data = mnist_data()\n",
    "# Create loader with data, so that we can iterate over it\n",
    "data_loader = torch.utils.data.DataLoader(data, batch_size=100, shuffle=True)\n",
    "# Num batches\n",
    "num_batches = len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-23T15:05:01.237598Z",
     "start_time": "2020-03-23T15:05:01.233993Z"
    }
   },
   "outputs": [],
   "source": [
    "#Create transformers image to tensor and tensor to image \n",
    "trans_0 = transforms.ToPILImage()\n",
    "trans_1 = transforms.ToTensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-23T15:08:33.879453Z",
     "start_time": "2020-03-23T15:07:08.746534Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMAGE NUMBER 10000\n",
      "IMAGE NUMBER 20000\n",
      "IMAGE NUMBER 30000\n",
      "IMAGE NUMBER 40000\n",
      "IMAGE NUMBER 50000\n",
      "IMAGE NUMBER 60000\n",
      "NON CHARGED 63 IMAGES\n"
     ]
    }
   ],
   "source": [
    "#Read the images and createds a list with the images in tensor format\n",
    "images_path = glob.glob(\"../Images/*jpg\")\n",
    "images_data = []\n",
    "no_charged = 0\n",
    "for idx, image in enumerate(images_path):\n",
    "    try:\n",
    "        img = Image.open(image)\n",
    "        img_tensor = trans_1(img)\n",
    "        images_data.append(img_tensor)\n",
    "    except:\n",
    "        no_charged += 1\n",
    "        pass\n",
    "    if idx % 10000 == 0 and idx > 0:\n",
    "        print('IMAGE NUMBER', idx)\n",
    "#Print the number of images non charged\n",
    "print('NON CHARGED', no_charged, 'IMAGES')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-23T15:09:46.761735Z",
     "start_time": "2020-03-23T15:09:46.753577Z"
    }
   },
   "outputs": [],
   "source": [
    "#Create loader with data, so that we can iterate over it\n",
    "data_loader = torch.utils.data.DataLoader(images_data, batch_size=100, shuffle=True)\n",
    "num_batches = len(data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GANs: Discrimator and Generator\n",
    "\n",
    "In this section we define the architecture of the two neural networks: the discriminator and the generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-23T15:10:00.990869Z",
     "start_time": "2020-03-23T15:10:00.983873Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x2cc51e690>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
